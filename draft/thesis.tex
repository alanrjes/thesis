% !TEX root = /home/Documents/thesis/draft/thesis.tex
\documentclass[12pt,twoside]{reedthesis}

\usepackage{graphicx,latexsym}
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace}
\usepackage{fancyvrb}
\usepackage[hyphens]{url}
\usepackage{rotating}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage[backend=biber, style=numeric, sorting=ynt]{biblatex}
\addbibresource{thesis.bib}

\lstset{	% for source code formatting
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\title{Simulating Granularity-Change Caching \\ for Realistic Scalable Systems}
\author{Alan R. Jessup}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2024}
\division{Mathematics and Natural Sciences}
\advisor{Charles McGuffey}
\department{Computer Science}

\setlength{\parskip}{0pt}
\begin{document}

\maketitle
\frontmatter % this stuff will be roman-numbered
\pagestyle{empty}

\tableofcontents

% If your abstract is longer than a page, there may be a formatting issue.
\chapter*{Abstract}

...

\mainmatter
\pagestyle{fancyplain}

\chapter{Introduction}

\section{A crash course in computer caching}

Computer memory is the fundamental component of digital systems that serves as a dynamic repository for data and instructions to be temporarily stored for quick access and retrieval during active processes. The drastically lower latency of memory requests compared to retrieval of data from a hard drive or SSD makes memory crucial to efficient performance of computers.

	\subsection*{The memory hierarchy}

	A computer's memory is divided into a \textit{memory hierarchy}, which arranges various types of computer memory in tiers based on side and access speed, with faster but smaller and more expensive layers of memory at the top of the hierarchy. When the processor requests data from memory, each layer of the hierarchy is checked in descending order until the data is located.

	\begin{figure}[h]
    	\centering
    	\includegraphics[width=3.5in]{figures/mem_hierarchy.jpg}
    	\caption{A diagram of the memory hierarchy.}
	\end{figure}

	\subsection*{Basics of caching}
	
	The purpose of caching is to reduce the number of accesses made to the main memory by storing data that is highly likely to be requested in the near future. When a piece of requested data is present in a cache, that is called a \textit{cache hit}; when it is absent, it's a \textit{miss}. Since caches are small and fast, intended to be accessed frequently at minimal cost, an important factor of cache design is how to best utilize the limited space they offer to optimize the \textit{hit rate}. The algorithms that caches use to decide which data to store are called \textit{replacement policies}, and are typically designed using \textit{spacial} and \textit{temporal locality}.

	\subsection*{Data locality}

	Locality is a characteristic of data which caches can use to predict what data is likely to be needed next, in order to store that data and reduce latency.
	
	Data with high temporal locality has been used recently; therefore, it is typically more likely to be needed again than any arbitrary other piece of data. Temporal locality is the more common characteristic used in caching, since it can be as simple to implement as a first-in-first-out (FIFO) queue, and is relatively effective on a wide variety of processes.
	
	Data with spacial locality is data that is located in memory physically near other useful data. Caches that make use of spacial locality typically load multiple adjacent lines from memory at the same time. This can be useful in specific cases where a large amount of data is stored sequentially, such as frames of a video. Spatial locality is much less well-studied than temporal locality, which the paper Beckmann et. al. aims to improve through introduction of the \textit{granularity-change caching problem}.

	\subsection*{Caches in realistic systems}

	Although [??upper/lower bounds/competitive ratios??] for cache models can be proven using theoretical analysis to give a general idea of how a cache model should perform, realistic estimations of cache performance in complex systems require testing to measure outcomes. The introduction of multiprocessing requires \textit{cache coherence protocols} to maintain consistency across shared memory resources, and multiprocessing adds variable of uncertainty into cache performance, since the structure of a specific processor's cache hierarchy or a particular program's implementation of parallelism can complicate cache behavior.

\section{The granularity-change caching problem}

The 2021 paper ``Spatial Locality and Granularity Change in Caching'' describes and provides a theoretical analysis of the Granularity-Change (GC) Caching Problem, which ``modifies the traditional caching setup by grouping data items into blocks, such that a cache can choose any subset of a block to load for the same cost as loading any individual item in the block'' \cite{beckmann}. This allows a cache to take advantage of both spatial and temporal locality, and creates an opportunity to study tradeoffs of cache space usage between the temporally determined \textit{item} cache versus the spatially oriented \textit{block} cache. For the purpose of implementation, Beckmann et. al. describes the deterministic replacement policy \textit{Item-Block Layer Partitioning}.

	\subsection*{Item-Block Layered Partitioning}

	Item-Block Layered Partitioning (IBLP) divides a cache into two virtual segments, one of item granularity and the other of block granularity.

	\begin{figure}[h]
		\centering
		\includegraphics[width=2.5in]{figures/IBLP.png}
		\caption{A diagram of a cache running IBLP \cite{beckmann}.}
	\end{figure}
	
	Beckmann et. al. describes the behavior of IBLP as: \begin{quote}
		The first layer, which serves each access to the cache, loads only the items that are accessed and evicts using the Least-Recently Used (LRU) replacement policy. The second layer, which only serves accesses that miss in the first layer, also uses the LRU policy for evictions, but loads and evicts at the granularity of entire blocks at a time. \cite{beckmann}
	\end{quote}
 
	IBLP models a subset of the granularity-change caching problem for which data is only loaded at the item or block granularity, and never as a subset of a block. This is sufficient for testing the viability of granularity-cache caching, since Beckmann et. al. finds that ``to achieve the best competitive ratio, one should load either an entire block or a single item, and nothing in between'' \cite{beckmann}.
	
	There is no requirement for the relative sizes $i$ and $b$ of the item and block caches in IBLP. Furthermore, Beckmann et. al. determines the optimal layer sizes in IBLP to be unknown, since they depend on the relative spatial and temporal locality of a particular trace, and recommends further analysis. This makes relative layer sizes a useful variable for furthering understanding of granularity-change caching.


\section{Prior work simulating GC caching}

In 2022, the Reed College thesis titled ``Simulating the Granularity-Change Caching Problem'', by Maxx Curtis, follows-up the theoretical work of Beckmann et. al. on the granularity-change caching problem by providing a foray into practical simulation of the proposed cache model. Curtis provides a survey of two systems simulators, Zsim and Gem5, and a presents a custom implementation of a block Cache in Gem5 \cite{curtis}.

	\subsection*{Block cache implementation}

	The block cache implemented by Curtis is built from the ground up as a new object, and thus is limited in what features of a Gem5 system it can be configured with. It operates on the level of handling packet streams between the CPU/caches, and facilitates the switch between item and block granularity at this level. 

	The ``Shortcomings'' section of Curtis explains that due to the time-consuming nature of this low-level implementation, the block cache is only usable in simple systems with a single cache level and ``Simple Timing CPU'', a ``unrealistic but very fast CPU model'' \cite{curtis}. Therefore, one of the priorities of my work here is to provide an implementation that allows for use in more realistic system configurations.

	Unfortunately the code from the Curtis thesis was unavailable, so continuing to build on this implementation of the block cache was not a possibility.

	\subsection*{Results of Curtis}

	Curtis tests three cache configurations against each other: a simple one-level cache of item granularity, a simple one-level cache using the custom block cache, and an IBLP cache constructed using a standard cache of item granularity in conjunction with a block cache. Three benchmarks are used: iterative merge sort (high in spacial locality), randomized subset-sum (high in temporal locality), and recursive merge sort (a combination of both forms of locality).
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=5in]{figures/curtis_caches.png}
		\caption{The three cache configurations tested by Curtis \cite{curtis}.}
	\end{figure}

	Both merge sort scripts result in a a hit rate near $100\%$ for all three cache types, and randomized subset-sum results in a hit rate near $99.5\%$ for the item and block caches and near $95.5\%$ for the IBLP cache \cite{curtis}. Since this does not line up with the expectation of poor performance of the item cache for high spacial locality and poor performance of the block cache for high temporal locality, the results of these tests are inconclusive.

	\begin{figure}[h]
		\centering
		\includegraphics[width=4.8in]{figures/curtis_cache_hit_rates.png}
		\caption{Hit rates resulting from tests of the three cache configurations \cite{curtis}.}
	\end{figure}

	Furthermore, looking specifically at the hit rates of the individual layers of the IBLP cache, the three test scripts do not support the expectation of a positive corelation between high spacial locality and high relative utilization of the block layer.

	\begin{figure}[h]
		\centering
		\includegraphics[width=4.5in]{figures/curtis_layer_hit_rates.png}
		\caption{Hit rates for the item and block layers of the IBLP cache \cite{curtis}.}
	\end{figure}

	Curtis suggests that these inconclusive results are due in part to the high granularity of the block layer in the IBLP cache, and insufficient memory workload of the test scripts compared to the caches' sizes. These factors are thus important to keep in mind when designing future tests to measure performance of granularity-change caches.

	\subsection*{Suggestions for future work}

	In conclusion, Curtis states: \begin{quote}
		At this stage of development, I recommend against using my implementation for further research into IBLP. ... If one only wanted to examine the viability of IBLP itself, then it would be fairly easy to use typical Cache objects to do so... by creating a system with two caches, one for each layer, that differ in granularity and experience little to no intermediate latency. \cite{curtis}
	\end{quote}

	Additionally, Curtis suggests working towards an implementation of a granularity-change cache model that is compatible with more realistic processors than Gem5's simple CPU models. Since Gem5 has a wide variety of available CPU models, this is a matter of compatibility with the full array of cache features in Gem5, including response queues and cache coherence.


\section{Approach}

With the results and suggestions of the Curtis thesis in mind, my approach to further research on the granularity-change caching problem is to build and test a modular implementation of IBLP using the existing Gem5 cache objects. Since the existing Gem5 caches are compatible with the full range of Gem5's features, an implementation using these caches as its foundation should be able to maintain these compatibilities, thereby eliminating the need for laborious re-implementation of Gem5's cache features from the ground up.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Cache Implementation in Gem5}

\section{Overview of Gem5}

Gem5 is an open-source, community-led computer computer systems simulation project that describes itself as: \begin{quote}
	...a modular platform for computer-system architecture research, encompassing system-level architecture as well as processor microarchitecture. \cite{gem5-about}
\end{quote}

Gem5 began as two separate projects, m5 at the University of Michigan and GEMS at the University of Wisconsin, until the two simulators were merged in 2011 \cite{gem5-about}.

	\subsection*{Caching subsystems}

	Due to its origins as a combination of m5 and GEMS, Gem5 contains two separate subsystems that can both be used to model caches. The \textit{classic} cache model from m5 provides simple, modular functionality for multi-level caches, but with an inflexible MOESI cache coherence protocol. The \textit{Ruby} cache model has more complex implementation details, with the ability to test customized cache coherence protocols.

\section{Initial classic cache implementation}

	I began by implementing a nestled cache structure using classic Gem5 caches. This involved first building a simple cache configuration following the Gem5 documentation's ``Getting Started'' tutorial, then modifying the cache bus configuration to support a nestled block layer and writing a wrapper program to perform trials and collect data from the simulation.

	\subsection*{Config files}

	A Gem5 configuration is defined by a python file in the ``configs'' directory. To run a config, the path to this file is given to the appropriate build command for the config's architecture, along with any arguments such as the path of the script to simulate.
	
	Contained within a config file are the commands to create a system object, set up a system, and start a simulation.

	\subsection*{System class}

	For the purpose of organizing and modularizing the facets of the Gem5 system configuration that are peripheral to the IBLP implementation, such as processor initialization and simulation execution, I implemented the system setup in a new class, \verb`StreamlinedSystem`. This class inherits the Gem5 \verb`System`' class and adds the following methods to take care of setting up the components of a typical system for simulation and piecing together the components of the IBLP cache:

	\begin{itemize}
		\item \verb`__init__(self)` sets up the system attributes that are needed for setting up other features: clock, voltage, memory mode and ranges, cpu, and busses for between L1 and L2 caches.
	
		\item \verb`setup_caches(self, cache_type)` sets up and connects busses to the caches: the L1 instruction and data caches, which have constant configurations, and the L2 cache, which is configured as either an Item, Block, or IBLP cache.
	
		\item \verb`setup_memory(self)` sets up the main memory and connects it to the L2 cache bus.
	
		\item \verb`run(self, binary)` sets the simulation up to run as a process, and runs it.
	\end{itemize}

	The specific commands for of most of these steps, except for \verb`setup_caches`, are adapted from the gem5 tutorial \cite{gem5-tutorial}.

	\subsection*{Cache objects}

	A basic classic cache configuration requires minor setup of cache objects and busses; this encompasses L1 data and instruction caches, which are specialized caches with ports specified in the System object, and a general L2 cache which I adapted into a nestled item and block cache:

	\begin{itemize}
		\item \verb`L1Cache`, \verb`DataCache`, and \verb`InstructionCache` are adapted from the gem5 basic caches tutorial to mimic the typical on-processor data and instruction caches in an ordinary system.
	
		\item \verb`L2Cache` contains the attributes of a typical L2 cache that are shared by both item and block caches.
	
		\item \verb`ItemCache` and \verb`BlockCache` inherit from \verb`L2Cache` and set up the structure for the two-part IBLP cache.
	
		\item \verb`IBLPCache(ItemCache)` simulates an IBLP cache by using an item cache, connected on the memory-side to a block cache, with attributes set so that there is minimal latency between these two caches. The method \verb`connectMemSideBus` is altered to connect the block layer to the memory bus, to imitate the way the memory connects to layers in the IBLP cache model.
	\end{itemize}

	\subsection*{Variable parameters}

	Since there are a number of parameters of the cache objects that should be adjusted and fine-tuned during testing, I created \verb`options.json` to be read by the file where the cache objects are defined, and used to set cache attributes accordingly.


	\subsection*{Wrapper}

	The wrapper file \verb`main.py` runs Gem5 with the config and measures the output written to \verb`stats.txt` at the end of each simulation. It's necessary that statistics be collected using a file outside the config because gem5 doesn't write to `stats.txt` until the simulation has terminated, and doesn't make these available during the running of a simulation.

	The wrapper takes the three arguments to customize trials: the path to the binary of the script to simulate, the cache models to be tested (out of Block, Item, and IBLP), and the number of trials to run of each cache type.

\section{Granularity limitations}

	The crucial limitation of my initial classic cache implementation is that it relies on Gem5 having a way, assumably through a cache object attribute, to specify the line size for any given layer of memory.

	Instead, Gem5 sets an attribute for line size at the level of the \verb`System` class. This prevents any possibly implementation of granularity-change caches in classic caches without modification of Gem5's \verb`System` and \verb`Cache` classes to allow specification of a larger line size for memory.

\section{A look at Ruby for changing granularity}

	Since my initial classic cache implementation failed to meet the requirements for simulating an IBLP cache, I turned to the Ruby cache system to see if the more detailed model had a way of overriding the system-wide line size requirement.

	\subsection*{...}

	...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Next section}

\section{Something or other}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}

\section{Results}

...

\section{Future work}

...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\chapter{Glossary}

\def\arraystretch{1.5}
\begin{tabular}{p{1.5in}p{3.8in}}
    \hline
    \textbf{Term}       & \textbf{Definition} \\
    \hline
	Block               & In the context of general caching, a standard amount of data. \\
	Block (GC-cache)	& In granularity-change caching, \textit{block} specifically refers to the lower-granularity line size. \\
	Cache coherence		& ... \\
	Granularity         & The size of a data \textit{block}, i.e. cache \textit{line}. A higher granularity has a larger block size, and lower vice versa. \\
	Competitive ratio	& ... \\
	Hit/miss rate 		& ... \\
    Item (GC-cache)  	& In GC-caching, \textit{item} refers to the higher-granularity line size. \\
    Line                & A portion of a cache that contains one \textit{block} of data. \\
	Memory hierarchy	& ... \\
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Source Code and Documentation}

\section{Simulation config}

\subsection*{Caches}
%\lstinputlisting{code/msi_caches.py}
\;\\

\subsection*{Runfile (Config)}
%\lstinputlisting{code/simple_ruby.py}
\;\\

\section{Test scripts}

...

\backmatter % backmatter makes the index and bibliography appear properly in the t.o.c...

\nocite{*}
\addcontentsline{toc}{chapter}{References}
\printbibliography[title=References]

\end{document}
